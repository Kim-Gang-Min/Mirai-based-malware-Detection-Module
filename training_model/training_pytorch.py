import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.optim as optim
import pickle

from sklearn.metrics import roc_curve, accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.preprocessing import Normalizer
import time
import seaborn as sns
import gc
import joblib
from sklearn.model_selection import train_test_split

from torch.utils.data import Dataset, DataLoader

import os

import time


class CustomDataset(Dataset):
    def __init__(self, X, Y=None):
        if self.Y is not None:
            self.Y = Y
        self.X = X

    def __getitem__(self, index):
        if self.Y is not None:
            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])
        else:
            return torch.Tensor(self.X[index])

    def __len__(self):
        return len(self.X)


class CustomLstmDataset(Dataset):
    def __init__(self, X, Y=None):
        self.Y = Y
        self.X = X

    def __getitem__(self, index):
        x_pd = self.X[index]
        input = torch.tensor(x_pd.values.tolist()).float()

        if self.Y is not None:
            y_pd = self.Y[index]
            label = torch.tensor(y_pd.values.tolist()).float()
            return input, label
        else:
            return input

    def __len__(self):
        return len(self.X)


class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(in_features=6, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=1),
            nn.ReLU()
        )

    def forward(self, x):
        output = self.linear_relu_stack(x)
        return output


class LstmModel(nn.Module):
    def __init__(self, b_size):
        super(LstmModel, self).__init__()
        self.lstm = nn.LSTM(input_size=7, hidden_size=256, num_layers=100, batch_first=True)
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(in_features=256, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=1),
            nn.ReLU()
        )

    def forward(self, x):
        hidden, _ = self.lstm(x)
        output = self.linear_relu_stack(hidden)
        return output


def data_scaling(x, y):
    norm = Normalizer()
    x = norm.fit_transform(x)
    norm.fit(x, y)
    x = pd.DataFrame(x, columns=['weak_port', 'proto', 'total_length', 'time', 'datarate', 'cnt'])
    return x, y


def get_x_y(org_csv):
    time_1 = time.time()
    #X_seq_list = []
    X_seq_list, y_seq_list = set_seq_x(org_csv['len_flag'], org_csv['attack'])
    y = pd.DataFrame(org_csv['attack'], columns=['attack'])
    X = pd.get_dummies(org_csv.drop(['ip', 'mac', 't_port','d_port', 'len_flag', 'attack'], axis=1))  # Remove feature that are not required for leaning
    X, y = data_scaling(X, y)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("x, y 추출 시간: " + str(time_interval))

    return X_seq_list, y_seq_list, X, y  # y_seq is the same as y


def set_seq_x(flag_len, y):  # csv: flag,length>flag,length
    list_df_flag_len_x = []
    list_df_flag_len_y = []

    for idx in range(0, len(flag_len)):  # get a col
        seq_list = flag_len[idx].split('>')  # split >: flag,length
        df_flag_len = pd.DataFrame(columns=['Flag_URG', 'Flag_ACK', 'Flag_PSH', 'Flag_RST', 'Flag_SYN', 'Flag_FIN', 'length'])
        df_y = pd.DataFrame(columns=['attack'])
        for element in range(0, 20):  # split flag, length
            if element < len(seq_list):
                tmp = seq_list[element].split(',')
                flag = tmp[0]
                flag_list = int_to_flag(int(flag))  # onehot encoding of flag
                flag_list.append(int(tmp[1]))
                df_flag_len.loc[len(df_flag_len)] = flag_list
                df_y.loc[len(df_flag_len)] = y[idx]
            else:
                df_flag_len.loc[len(df_flag_len)] = [0, 0, 0, 0, 0, 0, 0]
                df_y.loc[len(df_flag_len)] = 0
        list_df_flag_len_y.append(df_y)
        list_df_flag_len_x.append(df_flag_len)
        del df_flag_len

    return list_df_flag_len_x, list_df_flag_len_y


def int_to_flag(flag):
    tmp_df_seq = []
    cnt = 0b100000
    for i in range(6):
        if flag & cnt == 0:
            tmp_df_seq.append(0)
        else:
            tmp_df_seq.append(1)
        cnt = cnt >> 1

    return tmp_df_seq


def load_data(path, filename):
    csv_path = os.path.join(path, filename)
    return pd.read_csv(csv_path, low_memory=False)


def random_forest(x_rf, y_rf):
    x_train, x_test, y_train, y_test = train_test_split(x_rf, y_rf, test_size=0.4)
    print("------랜덤 포레스트 시작------")
    rf_clf = RandomForestClassifier(random_state=40, max_depth=3)

    time_1 = time.time()
    rf_clf.fit(x_train, y_train)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("교육 시간: " + str(time_interval))

    time_1 = time.time()
    pred = rf_clf.predict(x_test)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("예측 시간: " + str(time_interval))

    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred, pos_label='positive', average='micro')
    recall = recall_score(y_test, pred, pos_label='positive', average='micro')

    print("정확도: {:.3f}".format(accuracy))
    print("적합률: {:.3f}".format(precision))
    print("재현율: {:.3f}".format(recall))
    print("F1 점수: {:.3f}".format((2*precision*recall) / (precision+recall)))

    joblib.dump(rf_clf, 'C:/Users/KIM_GANG_MIN/Desktop/DataSet/new_Model/ran.h5')

    time_1 = time.time()
    ftr_importance_values = rf_clf.feature_importances_
    ftr_importance = pd.Series(ftr_importance_values, index=x_train.columns)
    ftr_top20 = ftr_importance.sort_values(ascending=False)[:15]
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("중요 피처 시각화 시간: " + str(time_interval))

    plt.figure(1, figsize=(8, 6))
    plt.title('Top 20 Feature Importance')
    sns.barplot(x=ftr_top20, y=ftr_top20.index)
    plt.figure(2, figsize=(8, 6))
    plt.show()


def simple_nn(x_nn, y_nn):
    x_train, x_test, y_train, y_test = train_test_split(x_nn, y_nn, test_size=0.4)
    x_train = x_train.values
    y_train = y_train.values
    x_test = x_test.values
    y_test = y_test.values

    time_1 = time.time()
    train_dataset = CustomDataset(x_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=0)

    test_dataset = CustomDataset(x_test)
    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=0)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Dataset 및 DataLoader 설정 시간: " + str(time_interval))

    time_1 = time.time()
    model = NeuralNetwork()
    optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=4,
                                                           threshold_mode='abs', min_lr=1e-8, verbose=True)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Optimizer 및 scheduler 설정 시간: " + str(time_interval))

    time_1 = time.time()
    model = train_nn(model, optimizer, train_loader, scheduler)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Train 시간: " + str(time_interval))

    time_1 = time.time()
    pred = inference(model, test_loader)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Predict 설정 시간: " + str(time_interval))

    result = nn_classification(pred)
    y_test = y_test.tolist()
    nn_accuracy(result, y_test)


def nn_accuracy(result, y_test):
    length = len(result)
    o = 0
    x = 0
    for i in range(0, length):
        if result[i] == y_test[i][0]:
            o = o + 1
        else:
            x = x + 1

    print("o = "+str(o) + ", x =  " + str(x) + ", accuracy: " + str(o/length))


def lstm_accuracy(result, y_test):
    length = len(result)
    o = 0
    x = 0
    for i in range(0, length):
        if result[i] == y_test[i]:
            o = o + 1
        else:
            x = x + 1

    print("o = " + str(o) + ", x =  " + str(x) + ", accuracy: " + str(o / length))


def nn_classification(pred):
    result = []
    for i in pred[0]:
        if i[0] < 0.5:
            result.append(0)
        elif i[0] < 1.5:
            result.append(1)
        elif i[0] < 2.5:
            result.append(2)
        else:
            result.append(-1)

    return result


def lstm_classification(pred):
    result = []
    for i in pred:
        if i < 0.5:
            result.append(0)
        elif i < 1.5:
            result.append(1)
        elif i < 2.5:
            result.append(2)
        else:
            result.append(-1)

    return result


def simple_lstm(x_seq, y_seq):
    x_train_list, x_test_list, y_train_list, y_test_list = train_test_split(x_seq, y_seq, test_size=0.4, random_state=42)
    b_size = len(x_seq)
    train_dataset = CustomLstmDataset(x_train_list, y_train_list)
    train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True, num_workers=0)

    time_1 = time.time()
    model = LstmModel(b_size)
    optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=4,
                                                           threshold_mode='abs', min_lr=1e-8, verbose=True)
    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Optimizer 및 scheduler 설정 시간: " + str(time_interval))

    time_1 = time.time()
    model = train_lstm(model, optimizer, train_loader, scheduler)

    time_2 = time.time()
    time_interval = time_2 - time_1
    print("Train 시간: " + str(time_interval))

    time_1 = time.time()
    test_dataset = CustomLstmDataset(x_test_list)
    test_loader = DataLoader(test_dataset, batch_size=b_size, shuffle=False, num_workers=0)

    pred = inference(model, test_loader)
    pred = sum(sum(pred, []), [])

    tmp = []
    for i in y_test_list:
        for j in i['attack']:
            tmp.append(j)

    time_2 = time.time()
    time_interval = time_2 - time_1

    print("Predict 설정 시간: " + str(time_interval))

    result = lstm_classification(pred)
    lstm_accuracy(result, tmp)


def train_nn(model, optimizer, train_loader, scheduler):
    criterion = nn.L1Loss()
    best_loss = 9999999
    best_model = None

    for epoch in range(1, 5):
        model.train()
        train_loss = []
        for X, Y in train_loader:
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, Y)
            loss.backward()
            optimizer.step()
            #scheduler.step()
            train_loss.append(loss.item())

    return model


def train_lstm(model, optimizer, train_loader, scheduler):
    criterion = nn.L1Loss()
    best_loss = 9999999
    best_model = None

    for epoch in range(1, 5):
        model.train()
        train_loss = []
        for X, Y in train_loader:
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, Y)
            loss.backward()
            optimizer.step()
#            scheduler.step()
            train_loss.append(loss.item())

    return model


def inference(model, test_loader):
    model.eval()
    pred = []
    with torch.no_grad():
        for X in test_loader:
            output = model(X)
            pred.extend(output.cpu().tolist())
    return pred


def data_write(pickle_name, data):
    with open(pickle_name, "wb") as fw:
        pickle.dump(data, fw)


def data_load(picke_name):
    with open(picke_name, "rb") as fr:
        data = pickle.load(fr)
    return data


if __name__ == '__main__':
    df = load_data('./', 'total_csv.csv')
    X_seq_list, Y_seq_list, X, y = get_x_y(df)

    data_write('X_seq_list.pickle', X_seq_list)
    data_write('Y_seq_list.pickle', Y_seq_list)
    data_write('X.pickle', X)
    data_write('y.pickle', y)

    #simple_nn(X, y)
    #random_forest(X, y)
    simple_lstm(X_seq_list, Y_seq_list)

